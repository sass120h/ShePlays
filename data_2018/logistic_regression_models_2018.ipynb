{"cells":[{"cell_type":"markdown","source":["#IST718 Project - Google and NCAA Women's Basketball Tournament Prediction\n\n#Logistic Regression Machine learning algorithm to build 20-year predictive model\n\n@authors\nSanjana Rajagopala,\nShefali Vajramatti,\nApoorva Rajendra Angre,\nSandya Madhavan"],"metadata":{}},{"cell_type":"code","source":["#IMPORT ALL THE REQUIRED PACKAGES\nimport pandas as pd\nfrom pyspark.ml import feature\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import classification\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import IntegerType\nimport numpy as np\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\nfrom pyspark.ml.feature import StringIndexer"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#################################################################################################################\n################################### Using 1998-2017 as training data ############################################"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Read the RegularCompact2018 CSV File\nCompactDF_2018 = spark.read.csv(\"/FileStore/tables/WRegularSeasonCompactResults_2018.csv\", header=True, inferSchema= True)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["display(CompactDF_2018)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Converting to pandas\nCompactDF_2018=CompactDF_2018.toPandas()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Define weights for the seeds of each team - Meaning keep the highest weight of 16 for the team with Seed 1\nweights_dict = {}\nj = 1\nfor i in range(16,0,-1):\n  weights_dict[j] = i\n  j+=1"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#PRE_PROCESSING THE DATAFRAME for 1998-2017- CompactDF_2018\n\ntemp_win = []\n#wseed_num = []\n#lseed_num = []\n#diff_seed = []\nloc_col = []\ndiff_score = []\nlow_team = []\nhigh_team = []\n\nfor row in CompactDF_2018.iterrows():\n  \n  team_1 = row[1]['WTeamID']\n  team_2 = row[1]['LTeamID']\n  loc_val = row[1]['WLoc']\n  #wseed = row[1]['WSeed']\n  #lseed =row[1]['LSeed']\n  \n  #Maintain the win column value as 1 if the team with lower teamID has won in the match\n  if(team_1<team_2):\n    temp_win.append(1)\n    low_team.append(team_1)\n    high_team.append(team_2)\n  else:\n    temp_win.append(0)\n    high_team.append(team_1)\n    low_team.append(team_2)\n    \n  #Give the highest weight when played in the home ground, least of outside home, medium vlaue otherwise   \n  if(loc_val == 'H'):\n    loc_col.append(3)\n  elif(loc_val == 'N'):\n    loc_col.append(2)\n  elif(loc_val=='A'):\n    loc_col.append(1)\n    \n  #Maintain the difference between seeds of the teams  \n  #temp_val = abs(weights_dict[int(wseed[1:])] - weights_dict[int(lseed[1:])])\n  #diff_seed.append(temp_val)\n  \n  #Maintain the column with difference between scores of the teams\n  diff_score.append(abs(row[1]['WScore'] - row[1]['LScore']))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#Add the above obtained lists as columns into the DF\nCompactDF_2018['WLProb'] = temp_win\n#WLCompact_2018['Seed_Diff'] = diff_seed\nCompactDF_2018['Loc'] = loc_col\nCompactDF_2018['Score_Diff'] = diff_score\nCompactDF_2018['Low_team'] = low_team\nCompactDF_2018['High_team'] = high_team"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Conversion into the Spark SQL Dataframe\nsqlCtx = SQLContext(sc)\nsql_compactDF_2018 = sqlCtx.createDataFrame(CompactDF_2018)\n\n#Rename the result column with the name label so that all the algorithms can be applied without any problems\nsql_compactDF_2018= sql_compactDF_2018.withColumnRenamed(\"WLProb\", \"label\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#To do\n#FEATURE ENGINEERING\n\n#Definition of new features from existing data\n\n#Obtain the totalMatches played and win percentage of the team in respective season\nwDF_2018 = sql_compactDF_2018.groupBy('WTeamID').agg(fn.sum('label').alias('won'), fn.count('Season').alias('WCount'))\nlDF_2018 = sql_compactDF_2018.groupBy('LTeamID').agg(fn.count('Season').alias('LCount'))\n\n#Rename and maintain a clean DF\nwDF_2018 = wDF_2018.selectExpr(\"WTeamID as teamID\", \"won\", \"WCount\")\nlDF_2018 = lDF_2018.selectExpr(\"LTeamID as teamID\", \"LCount\")\n\n#Create a DF of matches with the above combined details\nmatchDF_2018 = wDF_2018.join(lDF_2018, (wDF_2018.teamID== lDF_2018.teamID), how='right')\nmatchDF_2018 = matchDF_2018.withColumn(\"totalMatches\", sum([matchDF_2018[col] for col in ['WCount', 'LCount']]))\n\n#Computing the win percetage for the individaul teams\nmatchDF_2018=matchDF_2018.withColumn(\"winPercentage\", fn.col('WCount')/fn.col('totalMatches') )\n\n#Create Pandas DF only for this manipulation\nmatch_pd_DF_2018 = matchDF_2018.toPandas()\n#Renaming and selecting required data - avoiding redundancy\nmatch_pd_DF_2018 = match_pd_DF_2018.iloc[:,[0,6]]\nmatchDF_2018 = sqlCtx.createDataFrame(match_pd_DF_2018)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Joining Sql Compact with win percentage fields\nwinPercentage_DF = sql_compactDF_2018.join(matchDF_2018, (matchDF_2018.teamID== sql_compactDF_2018.WTeamID), how='left').select('DayNum', sql_compactDF_2018.Season, 'WTeamID', 'WScore', 'LTeamID', \"LScore\", 'NumOT', 'label', 'Loc', \"Score_Diff\",fn.col('winPercentage').alias('W_win_percentage'), 'Low_team') \n\nwinPercentage_DF = winPercentage_DF.join(matchDF_2018, (matchDF_2018.teamID== winPercentage_DF.LTeamID), how='left').select('DayNum', winPercentage_DF.Season, 'WTeamID', 'WScore', 'LTeamID', \"LScore\", 'NumOT', 'label', 'Loc' ,\"Score_Diff\",'W_win_percentage',fn.col('winPercentage').alias('L_win_percentage'), 'Low_team' )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#Filling NAs with zero\nwinPercentage_DF=winPercentage_DF.na.fill(0)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["trainedData1998_2017 = spark.read.csv(\"/FileStore/tables/98_17DF.csv\", header=True, inferSchema= True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(trainedData1998_2017)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Feature Definition and Vector Assembler creation\n\n##############################################   MODEL - 1 #####################################################\n#Initial Set of Features with only the simple columns\n#DayNum - With the higher day number means games played at later stages. Hence, add as feature so that it means a better performance\n#WTeamID -ID indicate the possibility in that match up\n#Score_Diff - Difference between win and lose scores of the match\n#NumOT - Number of Overtimes in the match\n#Loc - Played at home, outside or neither\nfeatureCols_1_2018 = [\"DayNum\", \"WTeamID\", \"Score_Diff\", \"Loc\", \"NumOT\"]\n\n#set the input and output column names**\nassembler_1_2018 = feature.VectorAssembler(inputCols = featureCols_1_2018, outputCol = \"features\")\n\n# Train a Logistic Regression model\nlogisticReg_1_2018 = LogisticRegression()\n\n# Chain vecAssembler and Logistic regression model  \npipeline_1_2018 = Pipeline(stages=[assembler_1_2018, logisticReg_1_2018])\n\n# Run stages in pipeline and train model\nmodel_1_2018 = pipeline_1_2018.fit(trainedData1998_2017)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#Determine the testing accuracy for the model performance\ntest_predictions_1_2018 = model_1_2018.transform(winPercentage_DF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(test_predictions_1_2018)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#Define the evaluator to obtain the areaUnderROC or the AUC score of the model\nevaluator = BinaryClassificationEvaluator()\n#Display the accuracies\nprint(\"The AUC metric for the testing dataset of model-1\", evaluator.evaluate(test_predictions_1_2018))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["############################### MODEL - 2 ##################################################\n\n#Defnition of Features - with all the match and team feautres, win and lose percetages and teh period win percetage feaures\nfeatureCols_2_2018 = [\"W_win_percentage\",\"L_win_percentage\",\"DayNum\",\"WTeamID\", \"Score_Diff\", \"Loc\", \"NumOT\"]\n\n#Logistic Regression for all features\nlogisticReg_2_2018 = LogisticRegression()\n\nassembler_2_2018=feature.VectorAssembler(inputCols=featureCols_2_2018,outputCol=\"features\")\n# Chain labelIndexer, vecAssembler and NBmodel in a \npipeline_2_2018 = Pipeline(stages=[ assembler_2_2018, logisticReg_2_2018])\n\n# Run stages in pipeline and train model\nmodel_2_2018 = pipeline_2_2018.fit(trainedData1998_2017)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Determine the testing accuracy for the model performance\ntest_predictions_2_2018 = model_2_2018.transform(winPercentage_DF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["#Display the accuracies\nprint(\"The AUC metric for the testing dataset of model-2\", evaluator.evaluate(test_predictions_2_2018))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#####################################################################################################################\n#################################### Using 2010-2017 as training data ###############################################\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Read the detailed dataset\nDetailedDF_2018=spark.read.csv(\"/FileStore/tables/WRegularSeasonDetailedResults_2018.csv\", header=True, inferSchema= True)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["DetailedDF_2018=DetailedDF_2018.toPandas()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#PRE_PROCESSING THE DATAFRAME for 2010-2017\n\ntemp_win = []\n#wseed_num = []\n#lseed_num = []\n#diff_seed = []\nloc_col = []\ndiff_score = []\nlow_team = []\nhigh_team = []\n\nfor row in DetailedDF_2018.iterrows():\n  \n  team_1 = row[1]['WTeamID']\n  team_2 = row[1]['LTeamID']\n  loc_val = row[1]['WLoc']\n  #wseed = row[1]['WSeed']\n  #lseed =row[1]['LSeed']\n  \n  #Maintain the win column value as 1 if the team with lower teamID has won in the match\n  if(team_1<team_2):\n    temp_win.append(1)\n    low_team.append(team_1)\n    high_team.append(team_2)\n  else:\n    temp_win.append(0)\n    high_team.append(team_1)\n    low_team.append(team_2)\n    \n  #Give the highest weight when played in the home ground, least of outside home, medium vlaue otherwise   \n  if(loc_val == 'H'):\n    loc_col.append(3)\n  elif(loc_val == 'N'):\n    loc_col.append(2)\n  elif(loc_val=='A'):\n    loc_col.append(1)\n    \n  #Maintain the difference between seeds of the teams  \n  #temp_val = abs(weights_dict[int(wseed[1:])] - weights_dict[int(lseed[1:])])\n  #diff_seed.append(temp_val)\n  \n  #Maintain the column with difference between scores of the teams\n  diff_score.append(abs(row[1]['WScore'] - row[1]['LScore']))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#Add the above obtained lists as columns into the DF\nDetailedDF_2018['WLProb'] = temp_win\n#WLCompact_2018['Seed_Diff'] = diff_seed\nDetailedDF_2018['Loc'] = loc_col\nDetailedDF_2018['Score_Diff'] = diff_score\nDetailedDF_2018['Low_team'] = low_team\nDetailedDF_2018['High_team'] = high_team"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#Conversion into the Spark SQL Dataframe\nsqlCtx = SQLContext(sc)\nsql_detailedDF_2018 = sqlCtx.createDataFrame(DetailedDF_2018)\n\n#Rename the result column with the name label so that all the algorithms can be applied without any problems\nsql_detailedDF_2018= sql_detailedDF_2018.withColumnRenamed(\"WLProb\", \"label\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["display(sql_detailedDF_2018)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#To do\n#FEATURE ENGINEERING\n\n#Definition of new features from existing data\n\n#Obtain the totalMatches played and win percentage of the team in respective season\ndtwDF_2018 = sql_detailedDF_2018.groupBy('WTeamID').agg(fn.sum('label').alias('won'), fn.count('Season').alias('WCount'))\ndtlDF_2018 = sql_detailedDF_2018.groupBy('LTeamID').agg(fn.count('Season').alias('LCount'))\n\n#Rename and maintain a clean DF\ndtwDF_2018 = dtwDF_2018.selectExpr(\"WTeamID as teamID\", \"won\", \"WCount\")\ndtlDF_2018 = dtlDF_2018.selectExpr(\"LTeamID as teamID\", \"LCount\")\n\n#Create a DF of matches with the above combined details\ndtmatchDF_2018 = dtwDF_2018.join(dtlDF_2018, (dtwDF_2018.teamID== dtlDF_2018.teamID), how='right')\ndtmatchDF_2018 = dtmatchDF_2018.withColumn(\"totalMatches\", sum([dtmatchDF_2018[col] for col in ['WCount', 'LCount']]))\n\n#Computing the win percetage for the individaul teams\ndtmatchDF_2018=dtmatchDF_2018.withColumn(\"winPercentage\", fn.col('WCount')/fn.col('totalMatches') )\n\n#Create Pandas DF only for this manipulation\ndtmatch_pd_DF_2018 = dtmatchDF_2018.toPandas()\n#Renaming and selecting required data - avoiding redundancy\ndtmatch_pd_DF_2018 = dtmatch_pd_DF_2018.iloc[:,[0,6]]\ndtmatchDF_2018 = sqlCtx.createDataFrame(dtmatch_pd_DF_2018)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#Display the DF to ensure the join has not missed any data rows and other details\ndisplay(dtmatchDF_2018)\n#Expect NaN because of the null values introduced during the join"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#Add the details from matchDF to the initial integrated DF\n\ndtwinPercentage_DF = sql_detailedDF_2018.join(dtmatchDF_2018, (dtmatchDF_2018.teamID== sql_detailedDF_2018.WTeamID), how='left').select('DayNum', sql_detailedDF_2018.Season, 'WTeamID', 'WScore', 'LTeamID', \"LScore\", 'NumOT', 'label', 'Loc',\"Score_Diff\",fn.col('winPercentage').alias('W_win_percentage'),'WFGM','WFGA','WFGM3','WFGA3','WFTM','WFTA','WOR','WDR','WAst','WTO','WStl','WBlk','WPF','LFGM', 'LFGA','LFGM3','LFGA3','LFTM','LFTA','LOR','LDR','LAst','LTO','LStl','LBlk','LPF', 'Low_team') \n\ndtwinPercentage_DF = dtwinPercentage_DF.join(dtmatchDF_2018, (dtmatchDF_2018.teamID== dtwinPercentage_DF.LTeamID), how='left').select('DayNum', dtwinPercentage_DF.Season, 'WTeamID', 'WScore', 'LTeamID', \"LScore\", 'NumOT', 'label', 'Loc' ,\"Score_Diff\",'W_win_percentage',fn.col('winPercentage').alias('L_win_percentage'), 'WFGM', 'WFGA','WFGM3','WFGA3','WFTM','WFTA','WOR','WDR','WAst','WTO','WStl','WBlk','WPF','LFGM', 'LFGA','LFGM3','LFGA3','LFTM','LFTA','LOR','LDR','LAst','LTO','LStl','LBlk','LPF','Low_team' )"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["#Removing null values \ndtwinPercentage_DF=dtwinPercentage_DF.na.fill(0)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["#Adding modified features to the initial dataframe\n\ntemp_DF = dtwinPercentage_DF.select('DayNum', 'WTeamID', 'WScore','LTeamID', \"LScore\", 'NumOT', 'label', 'Loc' ,\"Score_Diff\",'W_win_percentage', 'L_win_percentage', 'Season', 'Low_team',(fn.col('WFGM')/fn.col('WFGA')).alias('WGoals_ratio'),(fn.col('WFGM3')/fn.col('WFGA3')).alias('W3pointers_ratio'),(fn.col('WFTM')/fn.col('WFTA')).alias('WFreethrows_ratio'),(fn.col('WOR')+fn.col('WDR')+fn.col('WAst')+fn.col('WTO')+fn.col('WStl')+fn.col('WBlk')-fn.col('WPF')).alias('Win_accomplish'), (fn.col('LFGM')/fn.col('LFGA')).alias('LGoals_ratio'),(fn.col('LFGM3')/fn.col('LFGA3')).alias('L3pointers_ratio'),(fn.col('LFTM')/fn.col('LFTA')).alias('LFreethrows_ratio'),(fn.col('LOR')+fn.col('LDR')+fn.col('LAst')+fn.col('LTO')+fn.col('LStl')+fn.col('LBlk')-fn.col('LPF')).alias('Lose_accomplish'))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["dtwinPercentage_DF=temp_DF\n#Display final dataframe\ndisplay(dtwinPercentage_DF)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#Store as test data\ntestData_2018 = dtwinPercentage_DF\ntestData_2018=testData_2018.na.fill(0)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["trainedData2010_2017 = spark.read.csv(\"/FileStore/tables/2010_2017_traindata.csv\", header=True, inferSchema= True)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(trainedData2010_2017)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#Feature Definition and Vector Assembler creation\n\n##############################################   MODEL - 3 #####################################################\n#Initial Set of Features with only the simple columns\n#DayNum - With the higher day number means games played at later stages. Hence, add as feature so that it means a better performance\n#WTeamID and LTeamID - the IDs indicate the possibility in that match up\n#Score_Diff - Difference between win and lose scores of the match\n#Seed_Diff - Difference between seeds of the two playing teams\n#NUMOT - Number of Overtimes in the match\n#Loc - Played at home, outside or neither\nfeatureCols_3_2018 = [\"DayNum\", \"WTeamID\", \"Score_Diff\", \"Loc\", \"NumOT\"]\n\n#set the input and output column names**\nassembler_3_2018 = feature.VectorAssembler(inputCols = featureCols_3_2018, outputCol = \"features\")\n\n# Train a Logistic Regression model\nlogisticReg_3_2018 = LogisticRegression()\n\n# Chain vecAssembler and Logistic regression model  \npipeline_3_2018 = Pipeline(stages=[assembler_3_2018, logisticReg_3_2018])\n\n# Run stages in pipeline and train model\nmodel_3_2018 = pipeline_3_2018.fit(trainedData2010_2017)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#Determine the testing accuracy for the model performance\ntest_predictions_3_2018 = model_3_2018.transform(testData_2018)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["#Define the evaluator to obtain the areaUnderROC or the AUC score of the model\nevaluator = BinaryClassificationEvaluator()\n\n#Display the accuracies\nprint(\"The AUC metric for the testing dataset of model-3\", evaluator.evaluate(test_predictions_3_2018))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["#Checking the average prediction for the test data set ( A Balanced dataset)\ntest_predictions_3_2018.select(fn.avg('prediction')).show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["###############################MODEL - 4 ##################################################\n\n#Defnition of Features - with all the match and team feautres, win and lose percetages and teh period win percetage feaures\nfeatureCols_4_2018 = [\"W_win_percentage\",\"L_win_percentage\",\"DayNum\",\"WTeamID\", \"Score_Diff\", \"Loc\", \"NumOT\"]\n\n#Logistic Regression for all features\nlogisticReg_4_2018 = LogisticRegression()\n\nassembler_4_2018=feature.VectorAssembler(inputCols=featureCols_4_2018,outputCol=\"features\")\n# Chain labelIndexer, vecAssembler and NBmodel in a \npipeline_4_2018 = Pipeline(stages=[ assembler_4_2018, logisticReg_4_2018])\n\n# Run stages in pipeline and train model\nmodel_4_2018 = pipeline_4_2018.fit(trainedData2010_2017)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["#Transforming on test data\ntest_predictions_4_2018 = model_4_2018.transform(testData_2018)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["#Accuracy for test data- using all features\n#print(\"The AUC Metric for validation data set of model-3 \", evaluator.evaluate(val_predictions_3))\nprint(\"The AUC Metric for test data set of model-4 \", evaluator.evaluate(test_predictions_4_2018))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["test_predictions_4_2018.select(fn.avg('prediction')).show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["##### Model 5 ###########\nfeatureCols_5_2018 = [\"WTeamID\",\"W_win_percentage\", \"L_win_percentage\"]\n\n#set the input and output column names**\nassembler_5_2018 = feature.VectorAssembler(inputCols = featureCols_5_2018, outputCol = \"features\")\n\nlogisticReg_5_2018 = LogisticRegression()\n\n# Chain vecAssembler and Logistic regression model  \npipeline_5_2018 = Pipeline(stages=[assembler_5_2018, logisticReg_5_2018])\n\n# Run stages in pipeline and train model\nmodel_5_2018 = pipeline_5_2018.fit(trainedData2010_2017)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["test_predictions_5_2018 = model_5_2018.transform(testData_2018)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["print(\"The AUC Metric for test data set of model-5 \", evaluator.evaluate(test_predictions_5_2018))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["######## Model 6 ################\nfeatureCols_6_2018 = [\"WTeamID\",\"DayNum\", \"Score_Diff\", \"Loc\", \"NumOT\", \"W_win_percentage\", \"L_win_percentage\", \"WGoals_ratio\", 'W3pointers_ratio', 'WFreethrows_ratio', 'Win_accomplish', 'LGoals_ratio','L3pointers_ratio', 'LFreethrows_ratio', 'Lose_accomplish']\n#set the input and output column names**\nassembler_6_2018 = feature.VectorAssembler(inputCols = featureCols_6_2018, outputCol = \"features\")\n\nlogisticReg_6_2018 = LogisticRegression()\n\n# Chain vecAssembler and Logistic regression model  \npipeline_6_2018 = Pipeline(stages=[assembler_6_2018, logisticReg_6_2018])\n\n# Run stages in pipeline and train model\nmodel_6_2018 = pipeline_6_2018.fit(trainedData2010_2017)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["test_predictions_6_2018 = model_6_2018.transform(testData_2018)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["print(\"The AUC Metric for test data set of model-6 \", evaluator.evaluate(test_predictions_6_2018))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["#Inferences with pie chart representation"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["display(test_predictions_1_2018)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["results= test_predictions_3_2018\nresults=results.toPandas()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["diff_result= []\n\nfor row in results.iterrows():\n  diff_result.append(abs(row[1]['prediction'] - row[1]['label']))\n  diff_result"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["a=diff_result.count(0)\nb=diff_result.count(1)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.figure()\nlabels = 'correct prediction', 'wrong prediction'\nsizes = [a,b]\ncolors = ['gold', 'yellowgreen']\nexplode = (0.1, 0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n \nplt.axis('equal')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["loc_result= []\n\nfor row in results.iterrows():\n  loc_result.append(abs(row[1]['Loc']))\n  loc_result\n  \na=loc_result.count(3)\nb=loc_result.count(2)\nc=loc_result.count(1)\n  \nimport matplotlib.pyplot as plt\nplt.figure()\nlabels = 'Home', 'Neutral', 'Away'\nsizes = [a,b,c]\ncolors = ['gold', 'yellowgreen', 'blue']\nexplode = (0, 0, 0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n \nplt.axis('equal')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":59}],"metadata":{"name":"logistic_regression_models_2018","notebookId":273380659670552},"nbformat":4,"nbformat_minor":0}
